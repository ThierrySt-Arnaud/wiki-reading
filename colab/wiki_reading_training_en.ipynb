{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wiki-reading-training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThierrySt-Arnaud/wiki-reading/blob/colab-conversion/colab/wiki_reading_training_en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiWuQR3cXN5P",
        "colab_type": "code",
        "outputId": "cbb57f21-a2c7-472d-bde4-f949e9c27d9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile get_data.sh\n",
        "echo \"Downloading English WikiReading TensorFlow Records...\"\n",
        "\n",
        "CLOUD_STORAGE=https://storage.googleapis.com/wikireading\n",
        "\n",
        "DATA_FOLDER=data\n",
        "\n",
        "downloadlExtractDelete(){\n",
        "  wget -c ${CLOUD_STORAGE}/${1}\n",
        "  tar xvzf ${1} -C ${DATA_FOLDER} --skip-old-files\n",
        "  rm ${1}\n",
        "}\n",
        "\n",
        "mkdir ${DATA_FOLDER}\n",
        "downloadlExtractDelete \"train.tar.gz\" &\n",
        "downloadlExtractDelete \"validation.tar.gz\" &\n",
        "downloadlExtractDelete \"test.tar.gz\" &\n",
        "wget -P ${DATA_FOLDER} https://github.com/google-research-datasets/wiki-reading/blob/master/README.md\n",
        "wget -P ${DATA_FOLDER} ${CLOUD_STORAGE}/answer.vocab\n",
        "wget -P ${DATA_FOLDER} ${CLOUD_STORAGE}/document.vocab\n",
        "wget -P ${DATA_FOLDER} ${CLOUD_STORAGE}/raw_answer.vocab\n",
        "wget -P ${DATA_FOLDER} ${CLOUD_STORAGE}/type.vocab \n",
        "wget -P ${DATA_FOLDER} ${CLOUD_STORAGE}/character.vocab\n",
        "wait\n",
        "\n",
        "echo \"Done.\"\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing get_data.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9uGpgb4u40o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b8909f4-e940-4cb1-cdb8-246fc088d878"
      },
      "source": [
        "!chmod +x get_data.sh\n",
        "!./get_data.sh"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading English WikiReading TensorFlow Records...\n",
            "--2019-11-12 13:14:58--  https://storage.googleapis.com/wikireading/validation.tar.gz\n",
            "--2019-11-12 13:14:58--  https://storage.googleapis.com/wikireading/train.tar.gz\n",
            "--2019-11-12 13:14:58--  https://github.com/google-research-datasets/wiki-reading/blob/master/README.md\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... --2019-11-12 13:14:58--  https://storage.googleapis.com/wikireading/test.tar.gz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... Resolving github.com (github.com)... Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.140.128, 74.125.140.1282a00:1450:400c:c0c::80, 2a00:1450:400c:c07::80\n",
            "\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.140.128|:443... Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.140.128|:443... 140.82.118.4\n",
            "Connecting to github.com (github.com)|140.82.118.4|:443... 64.233.184.128, 2a00:1450:400c:c0b::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.184.128|:443... connected.\n",
            "connected.\n",
            "connected.\n",
            "connected.\n",
            "HTTP request sent, awaiting response... HTTP request sent, awaiting response... HTTP request sent, awaiting response... HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘data/README.md’\n",
            "\n",
            "\rREADME.md               [<=>                 ]       0  --.-KB/s               \rREADME.md               [ <=>                ]  76.61K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2019-11-12 13:14:58 (8.39 MB/s) - ‘data/README.md’ saved [78452]\n",
            "\n",
            "--2019-11-12 13:14:58--  https://storage.googleapis.com/wikireading/answer.vocab\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.184.128, 2a00:1450:400c:c0c::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.184.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 180297 (176K) [application/octet-stream]\n",
            "Saving to: ‘data/answer.vocab’\n",
            "\n",
            "\ranswer.vocab          0%[                    ]       0  --.-KB/s               \ranswer.vocab        100%[===================>] 176.07K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2019-11-12 13:14:58 (110 MB/s) - ‘data/answer.vocab’ saved [180297/180297]\n",
            "\n",
            "--2019-11-12 13:14:58--  https://storage.googleapis.com/wikireading/document.vocab\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.184.128, 2a00:1450:400c:c08::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.184.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3335783 (3.2M) [application/octet-stream]\n",
            "Saving to: ‘data/document.vocab’\n",
            "\n",
            "document.vocab      100%[===================>]   3.18M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2019-11-12 13:14:59 (223 MB/s) - ‘data/document.vocab’ saved [3335783/3335783]\n",
            "\n",
            "--2019-11-12 13:14:59--  https://storage.googleapis.com/wikireading/raw_answer.vocab\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.184.128, 2a00:1450:400c:c07::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.184.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46490325081 (43G) [application/gzip]\n",
            "Saving to: ‘train.tar.gz’\n",
            "\n",
            "train.tar.gz          0%[                    ]       0  --.-KB/s               200 OK\n",
            "Length: 2737187424 (2.5G) [application/gzip]\n",
            "Saving to: ‘test.tar.gz’\n",
            "\n",
            "test.tar.gz           0%[                    ]       0  --.-KB/s               200 OK\n",
            "Length: 5464558895 (5.1G) [application/gzip]\n",
            "Saving to: ‘validation.tar.gz’\n",
            "\n",
            "validation.tar.gz     0%[                    ]       0  --.-KB/s               200 OK\n",
            "Length: 38010403 (36M) [application/octet-stream]\n",
            "Saving to: ‘data/raw_answer.vocab’\n",
            "\n",
            "raw_answer.vocab    100%[===================>]  36.25M  46.4MB/s    in 0.8s    \n",
            "\n",
            "2019-11-12 13:15:00 (46.4 MB/s) - ‘data/raw_answer.vocab’ saved [38010403/38010403]\n",
            "\n",
            "test.tar.gz           0%[                    ]   4.01M  4.86MB/s               --2019-11-12 13:15:00--  https://storage.googleapis.com/wikireading/type.vocab\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.140.128, 2a00:1450:400c:c07::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.140.128|:443... connected.\n",
            "train.tar.gz          0%[                    ]  16.01M  12.8MB/s               200 OK\n",
            "Length: 1306 (1.3K) [application/octet-stream]\n",
            "Saving to: ‘data/type.vocab’\n",
            "\n",
            "type.vocab          100%[===================>]   1.28K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-11-12 13:15:00 (33.4 MB/s) - ‘data/type.vocab’ saved [1306/1306]\n",
            "\n",
            "test.tar.gz           0%[                    ]  22.17M  21.6MB/s               --2019-11-12 13:15:00--  https://storage.googleapis.com/wikireading/character.vocab\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.184.128, 2a00:1450:400c:c07::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.184.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 147317 (144K) [application/octet-stream]\n",
            "Saving to: ‘data/character.vocab’\n",
            "\n",
            "character.vocab     100%[===================>] 143.86K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2019-11-12 13:15:00 (133 MB/s) - ‘data/character.vocab’ saved [147317/147317]\n",
            "\n",
            "test.tar.gz         100%[===================>]   2.55G  46.0MB/s    in 76s     \n",
            "\n",
            "2019-11-12 13:16:15 (34.2 MB/s) - ‘test.tar.gz’ saved [2737187424/2737187424]\n",
            "\n",
            "test-00000-of-00015\n",
            "train.tar.gz          6%[>                   ]   2.81G  36.3MB/s    eta 20m 4s test-00001-of-00015\n",
            "train.tar.gz          7%[>                   ]   3.07G  35.4MB/s    eta 19m 51stest-00002-of-00015\n",
            "validation.tar.gz    58%[==========>         ]   2.96G  34.3MB/s    eta 70s    test-00003-of-00015\n",
            "validation.tar.gz    62%[===========>        ]   3.19G  35.3MB/s    eta 63s    test-00004-of-00015\n",
            "train.tar.gz          9%[>                   ]   3.91G  52.0MB/s    eta 18m 53stest-00005-of-00015\n",
            "train.tar.gz          9%[>                   ]   4.23G  44.3MB/s    eta 18m 22stest-00006-of-00015\n",
            "train.tar.gz         10%[=>                  ]   4.52G  43.7MB/s    eta 18m 2s test-00007-of-00015\n",
            "train.tar.gz         11%[=>                  ]   4.79G  42.9MB/s    eta 17m 52stest-00008-of-00015\n",
            "validation.tar.gz    85%[================>   ]   4.35G  33.6MB/s    eta 25s    test-00009-of-00015\n",
            "train.tar.gz         12%[=>                  ]   5.34G  37.4MB/s    eta 17m 25stest-00010-of-00015\n",
            "train.tar.gz         12%[=>                  ]   5.62G  43.2MB/s    eta 17m 10stest-00011-of-00015\n",
            "train.tar.gz         13%[=>                  ]   5.87G  36.0MB/s    eta 17m 1s test-00012-of-00015\n",
            "train.tar.gz         14%[=>                  ]   6.11G  40.6MB/s    eta 16m 54stest-00013-of-00015\n",
            "validation.tar.gz   100%[===================>]   5.09G  26.6MB/s    in 2m 47s  \n",
            "\n",
            "2019-11-12 13:17:46 (31.2 MB/s) - ‘validation.tar.gz’ saved [5464558895/5464558895]\n",
            "\n",
            "validation-00000-of-00015\n",
            "train.tar.gz         14%[=>                  ]   6.41G  29.6MB/s    eta 16m 48stest-00014-of-00015\n",
            "train.tar.gz         15%[==>                 ]   6.72G  36.2MB/s    eta 16m 48svalidation-00001-of-00015\n",
            "train.tar.gz         16%[==>                 ]   7.24G  43.0MB/s    eta 16m 20svalidation-00002-of-00015\n",
            "train.tar.gz         18%[==>                 ]   7.82G  49.7MB/s    eta 15m 57svalidation-00003-of-00015\n",
            "train.tar.gz         19%[==>                 ]   8.30G  41.9MB/s    eta 15m 37svalidation-00004-of-00015\n",
            "train.tar.gz         20%[===>                ]   8.75G  36.6MB/s    eta 15m 27svalidation-00005-of-00015\n",
            "train.tar.gz         20%[===>                ]   9.09G  32.0MB/s    eta 15m 26svalidation-00006-of-00015\n",
            "train.tar.gz         22%[===>                ]   9.61G  47.2MB/s    eta 15m 8s validation-00007-of-00015\n",
            "train.tar.gz         23%[===>                ]  10.13G  41.4MB/s    eta 14m 50svalidation-00008-of-00015\n",
            "train.tar.gz         24%[===>                ]  10.55G  32.1MB/s    eta 14m 39svalidation-00009-of-00015\n",
            "train.tar.gz         25%[====>               ]  10.95G  31.9MB/s    eta 14m 33svalidation-00010-of-00015\n",
            "train.tar.gz         26%[====>               ]  11.41G  41.9MB/s    eta 14m 18svalidation-00011-of-00015\n",
            "train.tar.gz         27%[====>               ]  11.91G  47.3MB/s    eta 14m 2s validation-00012-of-00015\n",
            "train.tar.gz         28%[====>               ]  12.54G  52.2MB/s    eta 13m 35svalidation-00013-of-00015\n",
            "train.tar.gz         29%[====>               ]  12.98G  37.4MB/s    eta 13m 22svalidation-00014-of-00015\n",
            "train.tar.gz        100%[===================>]  43.30G  52.2MB/s    in 17m 11s \n",
            "\n",
            "2019-11-12 13:32:10 (43.0 MB/s) - ‘train.tar.gz’ saved [46490325081/46490325081]\n",
            "\n",
            "train-00000-of-00150\n",
            "train-00001-of-00150\n",
            "train-00002-of-00150\n",
            "train-00003-of-00150\n",
            "train-00004-of-00150\n",
            "train-00005-of-00150\n",
            "train-00006-of-00150\n",
            "train-00007-of-00150\n",
            "train-00008-of-00150\n",
            "train-00009-of-00150\n",
            "train-00010-of-00150\n",
            "train-00011-of-00150\n",
            "train-00012-of-00150\n",
            "train-00013-of-00150\n",
            "train-00014-of-00150\n",
            "train-00015-of-00150\n",
            "train-00016-of-00150\n",
            "train-00017-of-00150\n",
            "train-00018-of-00150\n",
            "train-00019-of-00150\n",
            "train-00020-of-00150\n",
            "train-00021-of-00150\n",
            "train-00022-of-00150\n",
            "train-00023-of-00150\n",
            "train-00024-of-00150\n",
            "train-00025-of-00150\n",
            "train-00026-of-00150\n",
            "train-00027-of-00150\n",
            "train-00028-of-00150\n",
            "train-00029-of-00150\n",
            "train-00030-of-00150\n",
            "train-00031-of-00150\n",
            "train-00032-of-00150\n",
            "train-00033-of-00150\n",
            "train-00034-of-00150\n",
            "train-00035-of-00150\n",
            "train-00036-of-00150\n",
            "train-00037-of-00150\n",
            "train-00038-of-00150\n",
            "train-00039-of-00150\n",
            "train-00040-of-00150\n",
            "train-00041-of-00150\n",
            "train-00042-of-00150\n",
            "train-00043-of-00150\n",
            "train-00044-of-00150\n",
            "train-00045-of-00150\n",
            "train-00046-of-00150\n",
            "train-00047-of-00150\n",
            "train-00048-of-00150\n",
            "train-00049-of-00150\n",
            "train-00050-of-00150\n",
            "train-00051-of-00150\n",
            "train-00052-of-00150\n",
            "train-00053-of-00150\n",
            "train-00054-of-00150\n",
            "train-00055-of-00150\n",
            "train-00056-of-00150\n",
            "train-00057-of-00150\n",
            "train-00058-of-00150\n",
            "train-00059-of-00150\n",
            "train-00060-of-00150\n",
            "train-00061-of-00150\n",
            "train-00062-of-00150\n",
            "train-00063-of-00150\n",
            "train-00064-of-00150\n",
            "train-00065-of-00150\n",
            "train-00066-of-00150\n",
            "train-00067-of-00150\n",
            "train-00068-of-00150\n",
            "train-00069-of-00150\n",
            "train-00070-of-00150\n",
            "train-00071-of-00150\n",
            "train-00072-of-00150\n",
            "train-00073-of-00150\n",
            "train-00074-of-00150\n",
            "train-00075-of-00150\n",
            "train-00076-of-00150\n",
            "train-00077-of-00150\n",
            "train-00078-of-00150\n",
            "train-00079-of-00150\n",
            "train-00080-of-00150\n",
            "train-00081-of-00150\n",
            "train-00082-of-00150\n",
            "train-00083-of-00150\n",
            "train-00084-of-00150\n",
            "train-00085-of-00150\n",
            "train-00086-of-00150\n",
            "train-00087-of-00150\n",
            "train-00088-of-00150\n",
            "train-00089-of-00150\n",
            "train-00090-of-00150\n",
            "train-00091-of-00150\n",
            "train-00092-of-00150\n",
            "train-00093-of-00150\n",
            "train-00094-of-00150\n",
            "train-00095-of-00150\n",
            "train-00096-of-00150\n",
            "train-00097-of-00150\n",
            "train-00098-of-00150\n",
            "train-00099-of-00150\n",
            "train-00100-of-00150\n",
            "train-00101-of-00150\n",
            "train-00102-of-00150\n",
            "train-00103-of-00150\n",
            "train-00104-of-00150\n",
            "train-00105-of-00150\n",
            "train-00106-of-00150\n",
            "train-00107-of-00150\n",
            "train-00108-of-00150\n",
            "train-00109-of-00150\n",
            "train-00110-of-00150\n",
            "train-00111-of-00150\n",
            "train-00112-of-00150\n",
            "train-00113-of-00150\n",
            "train-00114-of-00150\n",
            "train-00115-of-00150\n",
            "train-00116-of-00150\n",
            "train-00117-of-00150\n",
            "train-00118-of-00150\n",
            "train-00119-of-00150\n",
            "train-00120-of-00150\n",
            "train-00121-of-00150\n",
            "train-00122-of-00150\n",
            "train-00123-of-00150\n",
            "train-00124-of-00150\n",
            "train-00125-of-00150\n",
            "train-00126-of-00150\n",
            "train-00127-of-00150\n",
            "train-00128-of-00150\n",
            "train-00129-of-00150\n",
            "train-00130-of-00150\n",
            "train-00131-of-00150\n",
            "train-00132-of-00150\n",
            "train-00133-of-00150\n",
            "train-00134-of-00150\n",
            "train-00135-of-00150\n",
            "train-00136-of-00150\n",
            "train-00137-of-00150\n",
            "train-00138-of-00150\n",
            "train-00139-of-00150\n",
            "train-00140-of-00150\n",
            "train-00141-of-00150\n",
            "train-00142-of-00150\n",
            "train-00143-of-00150\n",
            "train-00144-of-00150\n",
            "train-00145-of-00150\n",
            "train-00146-of-00150\n",
            "train-00147-of-00150\n",
            "train-00148-of-00150\n",
            "train-00149-of-00150\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh9-DuuAuXBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright 2016 The Google Inc. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"Utils for all models.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def resize_axis(tensor, axis, new_size, fill_value=0):\n",
        "  \"\"\"Truncates or pads a tensor to new_size on on a given axis.\n",
        "\n",
        "  Truncate or extend tensor such that tensor.shape[axis] == new_size. If the\n",
        "  size increases, the padding will be performed at the end, using fill_value.\n",
        "\n",
        "  Args:\n",
        "    tensor: The tensor to be resized.\n",
        "    axis: An integer representing the dimension to be sliced.\n",
        "    new_size: An integer or 0d tensor representing the new value for\n",
        "      tensor.shape[axis].\n",
        "    fill_value: Value to use to fill any new entries in the tensor. Will be\n",
        "      cast to the type of tensor.\n",
        "\n",
        "  Returns:\n",
        "    The resized tensor.\n",
        "  \"\"\"\n",
        "  tensor = tf.convert_to_tensor(tensor)\n",
        "  shape = tf.unstack(tf.shape(tensor))\n",
        "\n",
        "  pad_shape = shape[:]\n",
        "  pad_shape[axis] = tf.maximum(0, new_size - shape[axis])\n",
        "\n",
        "  shape[axis] = tf.minimum(shape[axis], new_size)\n",
        "  shape = tf.stack(shape)\n",
        "\n",
        "  resized = tf.concat(axis=axis,\n",
        "                      values=[\n",
        "      tf.slice(tensor, tf.zeros_like(shape), shape),\n",
        "      tf.fill(tf.stack(pad_shape), tf.cast(fill_value, tensor.dtype))\n",
        "  ])\n",
        "\n",
        "  # Update shape.\n",
        "  new_shape = tensor.get_shape().as_list()  # A copy is being made.\n",
        "  new_shape[axis] = new_size\n",
        "  resized.set_shape(new_shape)\n",
        "  return resized\n",
        "\n",
        "\n",
        "def prune_out_of_vocab_ids(sparse_ids, vocab_size):\n",
        "  \"\"\"Prunes out of vocabulary ids from given SparseTensor.\"\"\"\n",
        "  is_id_valid = tf.less(sparse_ids.values, vocab_size)\n",
        "  return tf.sparse_retain(sparse_ids, is_id_valid)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MqJFlSO0EAt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "outputId": "0cf62fa2-4128-4a02-f5b4-b6cc3e264ef2"
      },
      "source": [
        "# Copyright 2016 The Google Inc. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"Bag of embeddings model.\"\"\"\n",
        "import tensorflow.estimator as learn\n",
        "from tensorflow.data.experimental import AUTOTUNE\n",
        "from tensorflow.contrib import layers\n",
        "\n",
        "VOCAB_SIZE = 10000\n",
        "EMBED_DIM = 20\n",
        "ANSWER_DIM = 2 * EMBED_DIM\n",
        "ANSWER_NUM = 5000\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.01\n",
        "HIDDEN_SIZE = 128\n",
        "SPARSE_FEATURES = ['document_sequence', 'question_sequence']\n",
        "filename = \"data/train-*\"\n",
        "\n",
        "\n",
        "def input_fn():\n",
        "  features = {k: tf.VarLenFeature(dtype=tf.int64) for k in SPARSE_FEATURES}\n",
        "  features['answer_ids'] = tf.VarLenFeature(dtype=tf.int64)\n",
        "  files = tf.data.Dataset.list_files(file_pattern=filename)\n",
        "  dataset = files.interleave(tf.data.TFRecordDataset,\n",
        "                              cycle_length=AUTOTUNE,\n",
        "                              num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "  def parse_fn(serialized):\n",
        "    example = tf.io.parse_single_sequence_example(serialized=serialized,\n",
        "                                                  sequence_features=features)[1]\n",
        "    labels = example.pop('answer_ids')\n",
        "    labels = resize_axis(tf.sparse_tensor_to_dense(labels), 1, 1)\n",
        "    return example, labels\n",
        "\n",
        "  dataset = dataset.map(map_func=parse_fn, num_parallel_calls=AUTOTUNE)\n",
        "  dataset = dataset.batch(batch_size=BATCH_SIZE)\n",
        "  dataset = dataset.shuffle(buffer_size=BATCH_SIZE)\n",
        "  dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def bow_model(features, labels):\n",
        "  document = prune_out_of_vocab_ids(features['document_sequence'], VOCAB_SIZE)\n",
        "  question = prune_out_of_vocab_ids(features['question_sequence'], VOCAB_SIZE)\n",
        "  answers = tf.squeeze(tf.one_hot(labels, ANSWER_NUM, 1.0, 0.0),\n",
        "                       axis=[1])\n",
        "  embeddings = tf.get_variable('embeddings', [VOCAB_SIZE, EMBED_DIM])\n",
        "  doc_enc = layers.safe_embedding_lookup_sparse(\n",
        "      [embeddings], document, None, combiner='sum')\n",
        "  question_enc = layers.safe_embedding_lookup_sparse(\n",
        "      [embeddings], question, None, combiner='sum')\n",
        "  joint_enc = tf.concat(axis=1, values=[doc_enc, question_enc])\n",
        "  answer_embeddings = tf.get_variable(\n",
        "      'answer_embeddings', [ANSWER_DIM, ANSWER_NUM])\n",
        "  answer_biases = tf.get_variable('answer_biases', [ANSWER_NUM])\n",
        "  softmax, loss = tf.nn.softmax(\n",
        "      joint_enc, answers, answer_embeddings, answer_biases)\n",
        "  train_op = layers.optimize_loss(\n",
        "      loss, tf.contrib.framework.get_global_step(),\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      optimizer='Adam')\n",
        "  return softmax, loss, train_op\n",
        "\n",
        "\n",
        "def main():\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  estimator = learn.Estimator(\n",
        "    model_fn=bow_model,\n",
        "    model_dir=\"results/bow/\",\n",
        "  )\n",
        "  estimator.evaluate(input_fn=input_fn, steps=10000)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'results/bow/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff646380ac8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Could not find trained model in model_dir: results/bow/, running initialization to evaluate.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From <ipython-input-42-b71c64fb5166>:55: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-b71c64fb5166>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-b71c64fb5166>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"results/bow/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   )\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, input_fn, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    478\u001b[0m           \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m           \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m   def _actual_eval(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_actual_eval\u001b[0;34m(self, input_fn, strategy, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_convert_eval_steps_to_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    502\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         (scaffold, update_op, eval_dict, all_hooks) = (\n\u001b[0;32m--> 504\u001b[0;31m             self._evaluate_build_graph(input_fn, hooks, checkpoint_path))\n\u001b[0m\u001b[1;32m    505\u001b[0m         return self._evaluate_run(\n\u001b[1;32m    506\u001b[0m             \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_evaluate_build_graph\u001b[0;34m(self, input_fn, hooks, checkpoint_path)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m       (scaffold, evaluation_hooks, input_hooks, update_op, eval_dict) = (\n\u001b[0;32m-> 1511\u001b[0;31m           self._call_model_fn_eval(input_fn, self.config))\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0mglobal_step_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn_eval\u001b[0;34m(self, input_fn, config)\u001b[0m\n\u001b[1;32m   1545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1546\u001b[0m     estimator_spec = self._call_model_fn(\n\u001b[0;32m-> 1547\u001b[0;31m         features, labels, ModeKeys.EVAL, config)\n\u001b[0m\u001b[1;32m   1548\u001b[0m     eval_metric_ops = _verify_and_create_loss_metric(\n\u001b[1;32m   1549\u001b[0m         estimator_spec.eval_metric_ops, estimator_spec.loss)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-b71c64fb5166>\u001b[0m in \u001b[0;36mbow_model\u001b[0;34m(features, labels)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[0manswer_biases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'answer_biases'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mANSWER_NUM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   softmax, loss = tf.nn.softmax(\n\u001b[0;32m---> 55\u001b[0;31m       joint_enc, answers, answer_embeddings, answer_biases)\n\u001b[0m\u001b[1;32m     56\u001b[0m   train_op = layers.optimize_loss(\n\u001b[1;32m     57\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(logits, axis, name, dim)\u001b[0m\n\u001b[1;32m   2953\u001b[0m       \u001b[0mdimension\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2954\u001b[0m   \"\"\"\n\u001b[0;32m-> 2955\u001b[0;31m   \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeprecation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecated_argument_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"axis\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2956\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2957\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mdeprecated_argument_lookup\u001b[0;34m(new_name, new_value, old_name, old_value)\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       raise ValueError(\"Cannot specify both '%s' and '%s'\" %\n\u001b[0;32m--> 599\u001b[0;31m                        (old_name, new_name))\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mold_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnew_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot specify both 'dim' and 'axis'"
          ]
        }
      ]
    }
  ]
}