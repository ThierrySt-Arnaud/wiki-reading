{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wiki-reading-training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThierrySt-Arnaud/wiki-reading/blob/colab-conversion/jupyter/wiki_reading_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4zZH7wYss0X",
        "colab_type": "text"
      },
      "source": [
        "Create a script file from get_data.sh (necessary because %%bash magic will not update in real time)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiWuQR3cXN5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile get_data.sh\n",
        "echo \"Downloading English WikiReading TensorFlow Records...\"\n",
        "\n",
        "CLOUD_STORAGE=https://storage.googleapis.com/wikireading\n",
        "\n",
        "DATA_FOLDER=data\n",
        "\n",
        "downloadlExtractDelete(){\n",
        "  wget -c ${CLOUD_STORAGE}/${1}\n",
        "  tar xvzf ${1} -C ${DATA_FOLDER} --skip-old-files\n",
        "  rm ${1}\n",
        "}\n",
        "\n",
        "mkdir ${DATA_FOLDER}\n",
        "downloadlExtractDelete \"train.tar.gz\" &\n",
        "downloadlExtractDelete \"validation.tar.gz\" &\n",
        "downloadlExtractDelete \"test.tar.gz\" &\n",
        "wget -P ${DATA_FOLDER} https://github.com/google-research-datasets/wiki-reading/blob/master/README.md\n",
        "wget -P ${DATA_FOLDER} ${CLOUD_STORAGE}/answer.vocab\n",
        "wget -P ${DATA_FOLDER} ${CLOUD_STORAGE}/document.vocab\n",
        "wget -P ${DATA_FOLDER} ${CLOUD_STORAGE}/raw_answer.vocab\n",
        "wget -P ${DATA_FOLDER} ${CLOUD_STORAGE}/type.vocab \n",
        "wget -P ${DATA_FOLDER} ${CLOUD_STORAGE}/character.vocab\n",
        "wait\n",
        "\n",
        "echo \"Done.\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFElJIG0s7bL",
        "colab_type": "text"
      },
      "source": [
        "Add execution permission and bash file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9uGpgb4u40o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!chmod +x get_data.sh\n",
        "!./get_data.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQyuD-JQtd6Y",
        "colab_type": "text"
      },
      "source": [
        "Conversion of utils.py to a Jupyter notebook using Tensorflow r1.15"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh9-DuuAuXBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright 2016 The Google Inc. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"Utils for all models.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def resize_axis(tensor, axis, new_size, fill_value=0):\n",
        "  \"\"\"Truncates or pads a tensor to new_size on on a given axis.\n",
        "\n",
        "  Truncate or extend tensor such that tensor.shape[axis] == new_size. If the\n",
        "  size increases, the padding will be performed at the end, using fill_value.\n",
        "\n",
        "  Args:\n",
        "    tensor: The tensor to be resized.\n",
        "    axis: An integer representing the dimension to be sliced.\n",
        "    new_size: An integer or 0d tensor representing the new value for\n",
        "      tensor.shape[axis].\n",
        "    fill_value: Value to use to fill any new entries in the tensor. Will be\n",
        "      cast to the type of tensor.\n",
        "\n",
        "  Returns:\n",
        "    The resized tensor.\n",
        "  \"\"\"\n",
        "  tensor = tf.convert_to_tensor(tensor)\n",
        "  shape = tf.unstack(tf.shape(tensor))\n",
        "\n",
        "  pad_shape = shape[:]\n",
        "  pad_shape[axis] = tf.maximum(0, new_size - shape[axis])\n",
        "\n",
        "  shape[axis] = tf.minimum(shape[axis], new_size)\n",
        "  shape = tf.stack(shape)\n",
        "\n",
        "  resized = tf.concat(axis=axis,\n",
        "                      values=[\n",
        "      tf.slice(tensor, tf.zeros_like(shape), shape),\n",
        "      tf.fill(tf.stack(pad_shape), tf.cast(fill_value, tensor.dtype))\n",
        "  ])\n",
        "\n",
        "  # Update shape.\n",
        "  new_shape = tensor.get_shape().as_list()  # A copy is being made.\n",
        "  new_shape[axis] = new_size\n",
        "  resized.set_shape(new_shape)\n",
        "  return resized\n",
        "\n",
        "\n",
        "def prune_out_of_vocab_ids(sparse_ids, vocab_size):\n",
        "  \"\"\"Prunes out of vocabulary ids from given SparseTensor.\"\"\"\n",
        "  is_id_valid = tf.less(sparse_ids.values, vocab_size)\n",
        "  return tf.sparse_retain(sparse_ids, is_id_valid)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aElzjmW8uHnP",
        "colab_type": "text"
      },
      "source": [
        "Conversion of bow.py to a Jupyter notebook using Tensorflow r1.15"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MqJFlSO0EAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright 2016 The Google Inc. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"Bag of embeddings model.\"\"\"\n",
        "import tensorflow.estimator as learn\n",
        "from tensorflow.data.experimental import AUTOTUNE\n",
        "from tensorflow.contrib import layers\n",
        "\n",
        "VOCAB_SIZE = 10000\n",
        "EMBED_DIM = 20\n",
        "ANSWER_DIM = 2 * EMBED_DIM\n",
        "ANSWER_NUM = 5000\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.01\n",
        "HIDDEN_SIZE = 128\n",
        "SPARSE_FEATURES = ['document_sequence', 'question_sequence']\n",
        "filename = \"data/train-*\"\n",
        "\n",
        "\n",
        "def input_fn():\n",
        "  features = {k: tf.VarLenFeature(dtype=tf.int64) for k in SPARSE_FEATURES}\n",
        "  features['answer_ids'] = tf.VarLenFeature(dtype=tf.int64)\n",
        "  files = tf.data.Dataset.list_files(file_pattern=filename)\n",
        "  dataset = files.interleave(tf.data.TFRecordDataset,\n",
        "                              cycle_length=AUTOTUNE,\n",
        "                              num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "  def parse_fn(serialized):\n",
        "    example = tf.io.parse_single_sequence_example(serialized=serialized,\n",
        "                                                  sequence_features=features)[1]\n",
        "    labels = example.pop('answer_ids')\n",
        "    labels = resize_axis(tf.sparse_tensor_to_dense(labels), 1, 1)\n",
        "    return example, labels\n",
        "\n",
        "  dataset = dataset.map(map_func=parse_fn, num_parallel_calls=AUTOTUNE)\n",
        "  dataset = dataset.batch(batch_size=BATCH_SIZE)\n",
        "  dataset = dataset.shuffle(buffer_size=BATCH_SIZE)\n",
        "  dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def bow_model(features, labels):\n",
        "  document = prune_out_of_vocab_ids(features['document_sequence'], VOCAB_SIZE)\n",
        "  question = prune_out_of_vocab_ids(features['question_sequence'], VOCAB_SIZE)\n",
        "  answers = tf.squeeze(tf.one_hot(labels, ANSWER_NUM, 1.0, 0.0),\n",
        "                       axis=[1])\n",
        "  embeddings = tf.get_variable('embeddings', [VOCAB_SIZE, EMBED_DIM])\n",
        "  doc_enc = layers.safe_embedding_lookup_sparse(\n",
        "      [embeddings], document, None, combiner='sum')\n",
        "  question_enc = layers.safe_embedding_lookup_sparse(\n",
        "      [embeddings], question, None, combiner='sum')\n",
        "  joint_enc = tf.concat(axis=1, values=[doc_enc, question_enc])\n",
        "  answer_embeddings = tf.get_variable(\n",
        "      'answer_embeddings', [ANSWER_DIM, ANSWER_NUM])\n",
        "  answer_biases = tf.get_variable('answer_biases', [ANSWER_NUM])\n",
        "\n",
        "  # TODO: Convert tf.contrib.ops.softmax() to tf.nn.softmax()\n",
        "  softmax, loss = tf.nn.softmax(\n",
        "      joint_enc, answers, answer_embeddings, answer_biases)\n",
        "  train_op = layers.optimize_loss(\n",
        "      loss, tf.contrib.framework.get_global_step(),\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      optimizer='Adam')\n",
        "  return softmax, loss, train_op\n",
        "\n",
        "\n",
        "def main():\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  estimator = learn.Estimator(\n",
        "    model_fn=bow_model,\n",
        "    model_dir=\"results/bow/\",\n",
        "  )\n",
        "  estimator.evaluate(input_fn=input_fn, steps=10000)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}