{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wiki-fast-reading-keras-en.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThierrySt-Arnaud/wiki-reading/blob/colab-conversion/colab/wiki_fast_reading_keras_en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4zZH7wYss0X",
        "colab_type": "text"
      },
      "source": [
        "Create a script file from get_data.sh (necessary because %%bash magic will not update in real time)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiWuQR3cXN5P",
        "colab_type": "code",
        "outputId": "05d05555-3221-4ed9-9922-1129c46ee67e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile get_data.sh\n",
        "echo \"Downloading English WikiReading TensorFlow Records...\"\n",
        "\n",
        "CLOUD_STORAGE=https://storage.googleapis.com/wikireading\n",
        "\n",
        "DATA_FOLDER=data\n",
        "\n",
        "downloadlExtractDelete(){\n",
        "  wget -c ${CLOUD_STORAGE}/${1}\n",
        "  tar xvzf ${1} -C ${DATA_FOLDER} --skip-old-files\n",
        "  rm ${1}\n",
        "}\n",
        "\n",
        "mkdir ${DATA_FOLDER}\n",
        "#downloadlExtractDelete \"train.json.tar.gz\" &\n",
        "#downloadlExtractDelete \"validation.json.tar.gz\" &\n",
        "downloadlExtractDelete \"test.json.tar.gz\" &\n",
        "wget -P ${DATA_FOLDER} https://github.com/google-research-datasets/wiki-reading/blob/master/README.md\n",
        "wget -P ${DATA_FOLDER} ${CLOUD_STORAGE}/answer.vocab\n",
        "wget -P ${DATA_FOLDER} ${CLOUD_STORAGE}/document.vocab\n",
        "wget -P ${DATA_FOLDER} ${CLOUD_STORAGE}/raw_answer.vocab\n",
        "wget -P ${DATA_FOLDER} ${CLOUD_STORAGE}/type.vocab \n",
        "wget -P ${DATA_FOLDER} ${CLOUD_STORAGE}/character.vocab\n",
        "wait\n",
        "\n",
        "echo \"Done.\"\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing get_data.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFElJIG0s7bL",
        "colab_type": "text"
      },
      "source": [
        "Add execution permission and execute bash file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9uGpgb4u40o",
        "colab_type": "code",
        "outputId": "9959d9cb-be90-4a6b-fc2a-5496c78a9aee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!chmod +x get_data.sh\n",
        "!./get_data.sh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading English WikiReading TensorFlow Records...\n",
            "--2019-11-20 22:45:09--  https://github.com/google-research-datasets/wiki-reading/blob/master/README.md\n",
            "--2019-11-20 22:45:09--  https://storage.googleapis.com/wikireading/test.json.tar.gz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... 74.125.199.128, 2607:f8b0:400e:c09::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.199.128|:443... connected.\n",
            "HTTP request sent, awaiting response... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘data/README.md’\n",
            "\n",
            "README.md               [<=>                 ]       0  --.-KB/s               200 OK\n",
            "Length: 2934382808 (2.7G) [application/gzip]\n",
            "Saving to: ‘test.json.tar.gz’\n",
            "\n",
            "README.md               [ <=>                ]  76.55K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-11-20 22:45:10 (604 KB/s) - ‘data/README.md’ saved [78384]\n",
            "\n",
            "--2019-11-20 22:45:10--  https://storage.googleapis.com/wikireading/answer.vocab\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.142.128, 2607:f8b0:400e:c09::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.142.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 180297 (176K) [application/octet-stream]\n",
            "Saving to: ‘data/answer.vocab’\n",
            "\n",
            "answer.vocab        100%[===================>] 176.07K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2019-11-20 22:45:10 (163 MB/s) - ‘data/answer.vocab’ saved [180297/180297]\n",
            "\n",
            "--2019-11-20 22:45:10--  https://storage.googleapis.com/wikireading/document.vocab\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.199.128, 2607:f8b0:400e:c02::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.199.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3335783 (3.2M) [application/octet-stream]\n",
            "Saving to: ‘data/document.vocab’\n",
            "\n",
            "document.vocab      100%[===================>]   3.18M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-11-20 22:45:10 (207 MB/s) - ‘data/document.vocab’ saved [3335783/3335783]\n",
            "\n",
            "--2019-11-20 22:45:10--  https://storage.googleapis.com/wikireading/raw_answer.vocab\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.20.128, 2607:f8b0:400e:c09::80\n",
            "test.json.tar.gz      0%[                    ]   8.01M  12.7MB/s               connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 38010403 (36M) [application/octet-stream]\n",
            "Saving to: ‘data/raw_answer.vocab’\n",
            "\n",
            "raw_answer.vocab    100%[===================>]  36.25M  62.8MB/s    in 0.6s    \n",
            "\n",
            "2019-11-20 22:45:11 (62.8 MB/s) - ‘data/raw_answer.vocab’ saved [38010403/38010403]\n",
            "\n",
            "--2019-11-20 22:45:11--  https://storage.googleapis.com/wikireading/type.vocab\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.199.128, 2607:f8b0:400e:c09::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.199.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1306 (1.3K) [application/octet-stream]\n",
            "Saving to: ‘data/type.vocab’\n",
            "\n",
            "type.vocab          100%[===================>]   1.28K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-11-20 22:45:12 (29.6 MB/s) - ‘data/type.vocab’ saved [1306/1306]\n",
            "\n",
            "--2019-11-20 22:45:12--  https://storage.googleapis.com/wikireading/character.vocab\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.199.128, 2607:f8b0:400e:c09::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.199.128|:443... connected.\n",
            "test.json.tar.gz      2%[                    ]  72.01M  41.3MB/s               200 OK\n",
            "Length: 147317 (144K) [application/octet-stream]\n",
            "Saving to: ‘data/character.vocab’\n",
            "\n",
            "character.vocab     100%[===================>] 143.86K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2019-11-20 22:45:12 (155 MB/s) - ‘data/character.vocab’ saved [147317/147317]\n",
            "\n",
            "test.json.tar.gz    100%[===================>]   2.73G  67.8MB/s    in 47s     \n",
            "\n",
            "2019-11-20 22:45:57 (59.5 MB/s) - ‘test.json.tar.gz’ saved [2934382808/2934382808]\n",
            "\n",
            "test-00000-of-00015.json\n",
            "test-00001-of-00015.json\n",
            "test-00002-of-00015.json\n",
            "test-00003-of-00015.json\n",
            "test-00004-of-00015.json\n",
            "test-00005-of-00015.json\n",
            "test-00006-of-00015.json\n",
            "test-00007-of-00015.json\n",
            "test-00008-of-00015.json\n",
            "test-00009-of-00015.json\n",
            "test-00010-of-00015.json\n",
            "test-00011-of-00015.json\n",
            "test-00012-of-00015.json\n",
            "test-00013-of-00015.json\n",
            "test-00014-of-00015.json\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iliTaGTwKRls",
        "colab_type": "text"
      },
      "source": [
        "Python import statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaBNc9DBKQRd",
        "colab_type": "code",
        "outputId": "12d5a172-576c-47f1-d169-84e532dbe329",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import io, json, glob, random\n",
        "import numpy as np\n",
        "import gc\n",
        "from collections import deque\n",
        "from itertools import islice\n",
        "from multiprocessing import Pool\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import Sequence\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQILWZIXTbN0",
        "colab_type": "text"
      },
      "source": [
        "Define global constants and variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxRIMIEfTR1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = 20000\n",
        "NGRAM_VOCAB = 20000\n",
        "STOPWORDS = tuple(range(80)+[86, 88, 90, 93, 100, 102, 110, ])\n",
        "NGRAMS = 3\n",
        "EMBED_DIM = 32\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.01\n",
        "FEATURE = 'document_sequence'\n",
        "LABEL = 'answer_ids'\n",
        "PATH_PREFIX = \"data\"\n",
        "TRAIN_FILE = \"train\"\n",
        "VALIDATIION_FILE = \"validation\"\n",
        "TEST_FILE = \"test\"\n",
        "MAX_LENGTH = 512\n",
        "MAX_NGRAMS = 512\n",
        "MAX_ANSWERS = 128\n",
        "\n",
        "ngram_indices = {}\n",
        "total_samples = -1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fNur2QyTfFy",
        "colab_type": "text"
      },
      "source": [
        "Define ngram creation utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRvbAweXTY-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ngram_freq = \n",
        "\n",
        "class NgramExtractor(object):\n",
        "    def __init__(self, feature=FEATURE,\n",
        "                 vocab_size=VOCAB_SIZE, ngram_range=NGRAMS):\n",
        "        self.feature = feature\n",
        "        self.vocab_size = vocab_size\n",
        "        self.ngram_range = ngram_range\n",
        "\n",
        "    def __call__(self, filename):\n",
        "      print(f\"Getting ngram set from {filename}\")\n",
        "      with open(filename, 'r') as file:\n",
        "        sequences = [json.loads(line)[self.feature] for line in file]\n",
        "\n",
        "      sequences = prune_lists(sequences, self.vocab_size)\n",
        "\n",
        "      gc.collect()\n",
        "\n",
        "      fileset = set()\n",
        "      for sequence in sequences:\n",
        "        for i in range(2, self.ngram_range + 1):\n",
        "            new_set = set(zip(*[sequence[j:] for j in range(i)]))\n",
        "            fileset.update(new_set)\n",
        "      del sequences\n",
        "      gc.collect()\n",
        "\n",
        "      print(f\"Extracted {len(fileset)} ngrams from {filename}\")\n",
        "      return fileset\n",
        "\n",
        "\n",
        "def create_ngram_set(filenames, feature=FEATURE,\n",
        "                     vocab_size=VOCAB_SIZE, ngrams=NGRAMS):\n",
        "  \"\"\"\n",
        "  Extract a set of n-grams from a list of files containing feature where\n",
        "  feature is a list of ints.\n",
        "\n",
        "  >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
        "  {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
        "\n",
        "  >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
        "  [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
        "  \"\"\"\n",
        "  # Create set of unique n-grams from the training set.\n",
        "  ngram_set = set()\n",
        "  with Pool(2) as pool:\n",
        "    new_sets = pool.map(NgramExtractor(feature, vocab_size, ngrams), filenames)\n",
        "\n",
        "  for new_set in new_sets:\n",
        "    ngram_set.update(new_set)\n",
        "  print(f\"Found {len(ngram_set)} unique n-grams (n <= {ngrams})\")\n",
        "\n",
        "  # Dictionary mapping n-gram token to a unique integer.\n",
        "  # Integer values are greater than max_features in order\n",
        "  # to avoid collision with existing features.\n",
        "  start_index = vocab_size + 1\n",
        "  token_indice = {v: k for k, v in enumerate(start_index, ngram_set)}\n",
        "  return token_indice\n",
        "\n",
        "\n",
        "def prune_lists(sequences, vocab_size=VOCAB_SIZE):\n",
        "  \"\"\"Remove all values that do not fit in the vocabulary\n",
        "  \"\"\"\n",
        "  return [[x for x in sequence if x <= vocab_size] for sequence in sequences]\n",
        "\n",
        "\n",
        "def get_ngrams(sequences, token_indice=ngram_indices,\n",
        "               ngram_range=NGRAMS, max_ngrams=MAX_NGRAMS):\n",
        "  \"\"\"\n",
        "  Returns the list of ngrams of token_indice\n",
        "  found in each sequence of sequences.\n",
        "\n",
        "  Example: adding bi-gram\n",
        "  >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
        "  >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
        "  >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
        "  [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
        "\n",
        "  Example: adding tri-gram\n",
        "  >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
        "  >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
        "  >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
        "  [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 2018, 1337, 42]]\n",
        "  \"\"\"\n",
        "  new_sequences = []\n",
        "  for input_list in sequences:\n",
        "    new_list = []\n",
        "    for ngram_value in range(ngram_range+1, 2, -1):\n",
        "      if len(new_list) < max_ngrams:\n",
        "        for i in range(len(input_list[:]) - ngram_value + 1):\n",
        "          ngram = tuple(input_list[i:i + ngram_value])\n",
        "          if ngram in token_indice:\n",
        "            new_list.append(token_indice[ngram])\n",
        "            if len(new_list) == max_ngrams:\n",
        "              break\n",
        "      else:\n",
        "        break\n",
        "    new_sequences.append(new_list)\n",
        "\n",
        "  return new_sequences\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQyuD-JQtd6Y",
        "colab_type": "text"
      },
      "source": [
        "Definition of the DataGenerator for wiki-reading + fasttext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh9-DuuAuXBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataGenerator(Sequence):\n",
        "  \"\"\"Generates data for Keras\n",
        "  Sequence based data generator. Suitable for\n",
        "  building data generator for training.\n",
        "  \"\"\"\n",
        "  def __init__(self, feature=FEATURE, max_length=MAX_LENGTH,\n",
        "               label=LABEL, path_prefix=PATH_PREFIX,\n",
        "               file_prefix=TRAIN_FILE, ngrams=NGRAMS,\n",
        "               max_ngrams=MAX_NGRAMS, batch_size=BATCH_SIZE, \n",
        "               vocab_size=VOCAB_SIZE, max_answers=MAX_ANSWERS,\n",
        "               shuffle=True):\n",
        "\n",
        "    \"\"\"Initialization\n",
        "    :param features: features to use for classification\n",
        "    :param labels: labels to use for training or validation\n",
        "    :param file_prefix: type of files to extract from\n",
        "    :param batch_size: batch size at each iteration\n",
        "    :param vocab_size: max vocab id used\n",
        "    :param shuffle: True to shuffle label indexes after every epoch\n",
        "    \"\"\"\n",
        "\n",
        "    self.feature = feature\n",
        "    self.label = label\n",
        "    self.max_length = max_length\n",
        "    self.path_prefix = path_prefix\n",
        "    self.ngrams = ngrams\n",
        "    self.max_ngrams = 0\n",
        "    self.max_answers = max_answers\n",
        "    self.batch_size = batch_size\n",
        "    self.vocab_size = vocab_size\n",
        "    self.shuffle = shuffle\n",
        "    self.filenames = glob.glob1(path_prefix,f\"{file_prefix}*.json\")\n",
        "     \n",
        "    if self.shuffle:\n",
        "      random.shuffle(self.filenames)\n",
        "    self.file_list = deque(\n",
        "        [f\"{path_prefix}/{file}\" for file in self.filenames])\n",
        "    global total_samples\n",
        "    if total_samples < 0:\n",
        "      self.total_samples = self._get_total_samples()\n",
        "      total_samples = self.total_samples\n",
        "    else:\n",
        "      print(f\"Reusing previous value of total_samples: {total_samples}\")\n",
        "      self.total_samples = total_samples\n",
        "\n",
        "    global ngram_indices\n",
        "    if ngrams > 1:\n",
        "      self.max_ngrams = max_ngrams\n",
        "      if ngram_indices:\n",
        "        print(f\"Reusing previously generated set of n-grams\")\n",
        "      else:\n",
        "        ngram_indices = create_ngram_set(\n",
        "            self.file_list, feature=feature,\n",
        "            vocab_size=vocab_size, ngrams=ngrams)\n",
        "  \n",
        "    self.current_file = open(self.file_list.popleft())\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"Denotes the number of batches per epoch\n",
        "    :return: number of batches per epoch\n",
        "    \"\"\"\n",
        "    return self.total_samples // self.batch_size\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \"\"\"Generate one batch of data\n",
        "    :param index: index of the batch\n",
        "    :return: x (feature), y (labels)\n",
        "    \"\"\"\n",
        "    batch = self._generate_sample()\n",
        "\n",
        "    if self.shuffle:\n",
        "      random.shuffle(batch)\n",
        "\n",
        "    # Extract selected features and labels from json object\n",
        "    x, y = zip(*[[prune_lists(sample[self.feature], self.vocab_size),\n",
        "                  sample[self.label]] for sample in batch])\n",
        "    \n",
        "    x_num = np.zeros((self.batch_size, self.max_length+self.max_ngrams),\n",
        "                     dtype=int)\n",
        "    y_num = np.zeros((self.batch_size, self.max_answers), dtype=int)\n",
        "\n",
        "    for i in range(self.batch_size):\n",
        "      actual_length = min(self.max_length, len(x[i]))\n",
        "      actual_answers = min(self.max_answers, len(y[i]))\n",
        "      x_num[i][:actual_length] = x[i][:self.max_length]\n",
        "      y_num[i][:actual_answers] = y[i][:self.max_answers]\n",
        "\n",
        "    if self.ngrams > 1:\n",
        "      ngrams = get_ngrams(x, ngram_indices, ngram_range, self.max_ngrams)\n",
        "      for i in range(self.batch_size):\n",
        "        actual_ngrams = min(self.max_ngrams, len(ngrams[i]))\n",
        "        x_num[i][max_length:actual_ngrams] = ngrams[i][:self.max_ngrams]\n",
        "\n",
        "    return x_num, y_num\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    self.current_file.close()\n",
        "    if self.shuffle:\n",
        "      random.shuffle(self.filenames)\n",
        "    self.file_list = deque(\n",
        "        [f\"{self.path_prefix}/{file}\" for file in self.filenames])\n",
        "\n",
        "  def _generate_sample(self):\n",
        "    \"\"\"Generates data containing batch_size images\n",
        "    :param list_IDs_temp: list of label ids to load\n",
        "    :return: batch of images\n",
        "    \"\"\"\n",
        "    batch = []\n",
        "    for _ in range(self.batch_size):\n",
        "      try:\n",
        "        batch.append(json.loads(next(self.current_file)))\n",
        "      except (StopIteration, ValueError):\n",
        "        if self.file_list:\n",
        "          self.current_file.close()\n",
        "          self.current_file = open(self.file_list.popleft())\n",
        "          batch.append(json.loads(next(self.current_file)))\n",
        "    return batch\n",
        "\n",
        "  def _get_total_samples(self):\n",
        "    print(f\"Getting number of samples from all files\")\n",
        "\n",
        "    # Count all samples in specified files\n",
        "    with Pool(4) as pool:\n",
        "      samples_per_file = pool.map(rawgencount, self.file_list)\n",
        "    return sum(samples_per_file)\n",
        "\n",
        "def _make_gen(reader):\n",
        "  b = reader(1024 * 1024)\n",
        "  while b:\n",
        "    yield b\n",
        "    b = reader(1024*1024)\n",
        "\n",
        "def rawgencount(filename):\n",
        "  print(f\"Opening {filename}\")\n",
        "  with open(filename, 'rb') as f:\n",
        "    f_gen = _make_gen(f.raw.read)\n",
        "    return sum( buf.count(b'\\n') for buf in f_gen )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aElzjmW8uHnP",
        "colab_type": "text"
      },
      "source": [
        "Conversion of bow.py to a Jupyter notebook using Keras + Tensorflow 2.x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MqJFlSO0EAt",
        "colab_type": "code",
        "outputId": "1f167670-8d7e-4dfc-fec6-893aeb4cd6d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        }
      },
      "source": [
        "\"\"\"FastText Model.\"\"\"\n",
        "\n",
        "def main():\n",
        "  inputs = keras.Input(shape=(1,), name='input')\n",
        "  embedding = layers.Embedding(VOCAB_SIZE+len(ngram_indices),\n",
        "                               EMBED_DIM, mask_zero=True)(inputs)\n",
        "  average = layers.GlobalAveragePooling1D()(embedding)\n",
        "  outputs = layers.Dense(1, use_bias=True, activation=keras.activations.sigmoid,\n",
        "                         name='predictions')(average)\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "                loss=keras.losses.binary_crossentropy,\n",
        "                metrics=['acc'])\n",
        "  test_generator = DataGenerator(file_prefix='test')\n",
        "  model.fit_generator(test_generator, steps_per_epoch=100, verbose=2)\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting number of samples from all files\n",
            "Opening data/test-00014-of-00015.json\n",
            "Opening data/test-00012-of-00015.json\n",
            "Opening data/test-00002-of-00015.json\n",
            "Opening data/test-00006-of-00015.json\n",
            "Opening data/test-00013-of-00015.json\n",
            "Opening data/test-00008-of-00015.json\n",
            "Opening data/test-00010-of-00015.json\n",
            "Opening data/test-00011-of-00015.json\n",
            "Opening data/test-00003-of-00015.json\n",
            "Opening data/test-00009-of-00015.json\n",
            "Opening data/test-00005-of-00015.json\n",
            "Opening data/test-00007-of-00015.json\n",
            "Opening data/test-00000-of-00015.json\n",
            "Opening data/test-00001-of-00015.json\n",
            "Opening data/test-00004-of-00015.json\n",
            "Getting ngram set from data/test-00014-of-00015.json\n",
            "Getting ngram set from data/test-00002-of-00015.json\n",
            "Extracted 17080088 ngrams from data/test-00002-of-00015.json\n",
            "Getting ngram set from data/test-00006-of-00015.json\n",
            "Extracted 17141610 ngrams from data/test-00014-of-00015.json\n",
            "Getting ngram set from data/test-00012-of-00015.json\n",
            "Extracted 17076344 ngrams from data/test-00006-of-00015.json\n",
            "Extracted 17116924 ngrams from data/test-00012-of-00015.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ae1c1395ee4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-ae1c1395ee4f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_crossentropy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 metrics=['acc'])\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mtest_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-9dba0ae4750d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, feature, max_length, label, path_prefix, file_prefix, ngrams, max_ngrams, batch_size, vocab_size, max_answers, shuffle)\u001b[0m\n\u001b[1;32m     52\u001b[0m         ngram_indices = create_ngram_set(\n\u001b[1;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             vocab_size=vocab_size, ngrams=ngrams)\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-f91867e5ea20>\u001b[0m in \u001b[0;36mcreate_ngram_set\u001b[0;34m(filenames, feature, vocab_size, ngrams)\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0mngram_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mnew_sets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNgramExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mnew_set\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_sets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}