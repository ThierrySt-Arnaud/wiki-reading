{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of wiki-reading-training_keras_en.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThierrySt-Arnaud/wiki-reading/blob/colab-conversion/colab/wiki_reading_training_keras_en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4zZH7wYss0X",
        "colab_type": "text"
      },
      "source": [
        "Create a script file from get_data.sh (necessary because %%bash magic will not update in real time)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiWuQR3cXN5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile get_data.sh\n",
        "echo \"Downloading English WikiReading TensorFlow Records...\"\n",
        "\n",
        "CLOUD_STORAGE=https://storage.googleapis.com/wikireading\n",
        "\n",
        "DATA_FOLDER=data\n",
        "\n",
        "downloadlExtractDelete(){\n",
        "  wget -c ${CLOUD_STORAGE}/${1}\n",
        "  tar xvzf ${1} -C ${DATA_FOLDER} --skip-old-files\n",
        "  rm ${1}\n",
        "}\n",
        "\n",
        "mkdir ${DATA_FOLDER}\n",
        "#downloadlExtractDelete \"train.json.tar.gz\" &\n",
        "#downloadlExtractDelete \"validation.json.tar.gz\" &\n",
        "downloadlExtractDelete \"test.json.tar.gz\" &\n",
        "wget -P ${DATA_FOLDER} https://github.com/google-research-datasets/wiki-reading/blob/master/README.md\n",
        "wget -P ${DATA_FOLDER} https://raw.githubusercontent.com/ThierrySt-Arnaud/wiki-reading/colab-conversion/data/stopwords.json\n",
        "wget -P ${DATA_FOLDER} https://raw.githubusercontent.com/ThierrySt-Arnaud/wiki-reading/colab-conversion/data/3-grams-test.pkl\n",
        "wget -P ${DATA_FOLDER} ${CLOUD_STORAGE}/answer.vocab\n",
        "wget -P ${DATA_FOLDER} ${CLOUD_STORAGE}/document.vocab\n",
        "wget -P ${DATA_FOLDER} ${CLOUD_STORAGE}/raw_answer.vocab\n",
        "wget -P ${DATA_FOLDER} ${CLOUD_STORAGE}/type.vocab \n",
        "wget -P ${DATA_FOLDER} ${CLOUD_STORAGE}/character.vocab\n",
        "wait\n",
        "\n",
        "echo \"Done.\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFElJIG0s7bL",
        "colab_type": "text"
      },
      "source": [
        "Add execution permission and execute bash file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9uGpgb4u40o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!chmod +x get_data.sh\n",
        "!./get_data.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iliTaGTwKRls",
        "colab_type": "text"
      },
      "source": [
        "Python import statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaBNc9DBKQRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import io, json, glob, random, gc, pickle\n",
        "import numpy as np\n",
        "from os import path\n",
        "from collections import deque, Counter\n",
        "from multiprocessing import Pool\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "%load_ext tensorboard\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import Sequence\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQILWZIXTbN0",
        "colab_type": "text"
      },
      "source": [
        "Define global constants and variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxRIMIEfTR1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = 32768\n",
        "NGRAM_VOCAB = 16384\n",
        "ANSWER_VOCAB = 8192\n",
        "NGRAMS = 3\n",
        "EMBED_DIM = 32\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "FEATURE = 'document_sequence'\n",
        "LABEL = 'answer_ids'\n",
        "PATH_PREFIX = \"data\"\n",
        "TRAIN_FILE = \"train\"\n",
        "VALIDATION_FILE = \"validation\"\n",
        "TEST_FILE = \"test\"\n",
        "MAX_LENGTH = 8192\n",
        "MAX_NGRAMS = 2048\n",
        "TARGET = TEST_FILE\n",
        "LOG_DIR = \"logs/fit/\"\n",
        "NGRAM_FILE = f\"{PATH_PREFIX}/{NGRAMS}-grams-{TARGET}.pkl\"\n",
        "STOPWORD_FILE = f\"{PATH_PREFIX}/stopwords.json\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBhwmwDWYdAr",
        "colab_type": "text"
      },
      "source": [
        "Define global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khYL1XPkYcI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(STOPWORD_FILE) as stopword_file:\n",
        "  stopwords = [x for x in json.load(stopword_file) if x < VOCAB_SIZE]\n",
        "\n",
        "ngram_indices = {}\n",
        "ngram_list = []\n",
        "if NGRAMS > 1:\n",
        "  if path.isfile(NGRAM_FILE):\n",
        "    with open(NGRAM_FILE, 'rb') as ngram_file:\n",
        "      ngram_list = pickle.load(ngram_file)[:NGRAM_VOCAB]\n",
        "  else:\n",
        "    print(f\"{NGRAM_FILE} not found. N-grams will need to be generated\")\n",
        "    \n",
        "total_samples = -1 # 941280 for test files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fNur2QyTfFy",
        "colab_type": "text"
      },
      "source": [
        "Define ngram creation utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRvbAweXTY-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NgramExtractor(object):\n",
        "    def __init__(self, feature=FEATURE, ngram_range=NGRAMS):\n",
        "        self.feature = feature\n",
        "        self.ngram_range = ngram_range\n",
        "\n",
        "    def __call__(self, filename):\n",
        "        print(f\"Getting ngram set from {filename}\")\n",
        "        with open(filename, 'r') as file:\n",
        "            sequences = [json.loads(line)[self.feature] for line in file]\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "        fileset = Counter()\n",
        "        for sequence in sequences:\n",
        "            for i in range(2, self.ngram_range + 1):\n",
        "                ngrams = zip(*[sequence[j:] for j in range(i)])\n",
        "                for ngram in ngrams:\n",
        "                    if all(x not in stopwords for x in ngram):\n",
        "                        fileset[ngram] += 1\n",
        "\n",
        "        del sequences\n",
        "        gc.collect()\n",
        "\n",
        "        print(f\"Extracted {len(fileset)} ngrams from {filename}\")\n",
        "        most_common = fileset.most_common(250000)\n",
        "        del fileset\n",
        "        gc.collect()\n",
        "        return most_common\n",
        "\n",
        "\n",
        "def create_ngram_set(filenames, feature=FEATURE, ngrams=NGRAMS):\n",
        "    \"\"\"\n",
        "    Extract a set of n-grams from a list of files containing feature where\n",
        "    feature is a list of ints.\n",
        "\n",
        "    create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
        "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
        "\n",
        "    create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
        "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create set of unique n-grams from the training set.\n",
        "    with Pool() as pool:\n",
        "        new_sets = pool.map(NgramExtractor(feature, ngrams), filenames)\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    ngram_freq = Counter()\n",
        "    for new_set in new_sets:\n",
        "        for ngram in new_set:\n",
        "            ngram_freq[ngram[0]] += ngram[1]\n",
        "\n",
        "    del new_sets\n",
        "    gc.collect()\n",
        "\n",
        "    sorted_tokens = [x for (x, v) in ngram_freq.most_common(500000)]\n",
        "\n",
        "    del ngram_freq\n",
        "    gc.collect()\n",
        "\n",
        "    return sorted_tokens\n",
        "\n",
        "\n",
        "def get_ngrams(sequences, token_indice=ngram_indices,\n",
        "               ngram_range=NGRAMS, max_ngrams=MAX_NGRAMS):\n",
        "  \"\"\"\n",
        "  Returns the list of ngrams of token_indice\n",
        "  found in each sequence of sequences.\n",
        "\n",
        "  Example: adding bi-gram\n",
        "  >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
        "  >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
        "  >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
        "  [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
        "\n",
        "  Example: adding tri-gram\n",
        "  >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
        "  >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
        "  >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
        "  [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 2018, 1337, 42]]\n",
        "  \"\"\"\n",
        "  new_sequences = []\n",
        "  for input_list in sequences:\n",
        "    new_list = []\n",
        "    for ngram_value in range(ngram_range+1, 2, -1):\n",
        "      if len(new_list) < max_ngrams:\n",
        "        for i in range(len(input_list[:]) - ngram_value + 1):\n",
        "          ngram = tuple(input_list[i:i + ngram_value])\n",
        "          if ngram in token_indice:\n",
        "            new_list.append(token_indice[ngram])\n",
        "            if len(new_list) == max_ngrams:\n",
        "              break\n",
        "      else:\n",
        "        break\n",
        "    new_sequences.append(new_list)\n",
        "\n",
        "  return new_sequences\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQyuD-JQtd6Y",
        "colab_type": "text"
      },
      "source": [
        "Definition of the DataGenerator for wiki-reading + fasttext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh9-DuuAuXBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataGenerator(Sequence):\n",
        "  \"\"\"Generates data for Keras\n",
        "  Sequence based data generator. Suitable for\n",
        "  building data generator for training.\n",
        "  \"\"\"\n",
        "  def __init__(self, feature=FEATURE, max_length=MAX_LENGTH,\n",
        "               label=LABEL, path_prefix=PATH_PREFIX,\n",
        "               file_prefix=TRAIN_FILE, ngrams=NGRAMS,\n",
        "               max_ngrams=MAX_NGRAMS, batch_size=BATCH_SIZE, \n",
        "               vocab_size=VOCAB_SIZE, answer_vocab=ANSWER_VOCAB,\n",
        "               shuffle=True):\n",
        "\n",
        "    \"\"\"Initialization\n",
        "    :param features: features to use for classification\n",
        "    :param labels: labels to use for training or validation\n",
        "    :param file_prefix: type of files to extract from\n",
        "    :param batch_size: batch size at each iteration\n",
        "    :param vocab_size: max vocab id used\n",
        "    :param shuffle: True to shuffle label indexes after every epoch\n",
        "    \"\"\"\n",
        "\n",
        "    self.feature = feature\n",
        "    self.label = label\n",
        "    self.max_length = max_length\n",
        "    self.path_prefix = path_prefix\n",
        "    self.ngrams = ngrams\n",
        "    self.max_ngrams = 0\n",
        "    self.answer_vocab = answer_vocab\n",
        "    self.batch_size = batch_size\n",
        "    self.vocab_size = vocab_size + len(stopwords)\n",
        "    self.shuffle = shuffle\n",
        "    self.filenames = glob.glob1(path_prefix,f\"{file_prefix}*.json\")\n",
        "    self.file_list = deque(\n",
        "        [f\"{path_prefix}/{file}\" for file in self.filenames])\n",
        "    global total_samples\n",
        "    if total_samples < 0:\n",
        "      self.total_samples = self._get_total_samples()\n",
        "      total_samples = self.total_samples\n",
        "    else:\n",
        "      print(f\"Reusing previous value of total_samples: {total_samples}\")\n",
        "      self.total_samples = total_samples\n",
        "\n",
        "    global ngram_indices, ngram_list\n",
        "    if ngrams > 1:\n",
        "      self.max_ngrams = max_ngrams\n",
        "      if not ngram_list:\n",
        "        ngram_list = create_ngram_sets(self.file_list, feature=feature,\n",
        "                                       ngrams=ngrams)\n",
        "      ngram_indices = {k: v for (v, k) in enumerate(ngram_list,\n",
        "                                                    self.vocab_size)}\n",
        "  \n",
        "    if self.shuffle:\n",
        "      random.shuffle(self.file_list)\n",
        "    self.current_file = open(self.file_list[0])\n",
        "    self.file_list.rotate()\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"Denotes the number of batches per epoch\n",
        "    :return: number of batches per epoch\n",
        "    \"\"\"\n",
        "    return self.total_samples // self.batch_size\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \"\"\"Generate one batch of data\n",
        "    :param index: index of the batch\n",
        "    :return: x (feature), y (labels)\n",
        "    \"\"\"\n",
        "    batch = self._generate_sample()\n",
        "\n",
        "    if self.shuffle:\n",
        "      random.shuffle(batch)\n",
        "\n",
        "    # Extract selected features and labels from json object\n",
        "    x, y = zip(*[[self.prune_sample(sample[self.feature])[:self.max_length],\n",
        "                  self.prune_label(sample[self.label])] for sample in batch])\n",
        "    \n",
        "    x_num = np.zeros((self.batch_size, self.max_length+self.max_ngrams),\n",
        "                     dtype=int)\n",
        "    y_num = np.zeros((self.batch_size, self.answer_vocab), dtype=bool)\n",
        "\n",
        "    for i in range(self.batch_size):\n",
        "      actual_length = min(self.max_length, len(x[i]))\n",
        "      x_num[i][:actual_length] = x[i]\n",
        "      for a in y[i]:\n",
        "        y_num[i][a-1] = True\n",
        "\n",
        "    if self.ngrams > 1:\n",
        "      sample_ngrams = get_ngrams(x, ngram_indices, self.ngrams, self.max_ngrams)\n",
        "      for i in range(self.batch_size):\n",
        "        actual_ngrams = min(self.max_ngrams, len(sample_ngrams[i]))\n",
        "        x_num[i][self.max_length:self.max_length+actual_ngrams]\\\n",
        "          = sample_ngrams[i][:self.max_ngrams]\n",
        "\n",
        "    return x_num, y_num\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    self.current_file.close()\n",
        "    if self.shuffle:\n",
        "      random.shuffle(self.file_list)\n",
        "    self.current_file = open(self.file_list[0])\n",
        "    self.file_list.rotate()\n",
        "    \n",
        "\n",
        "  def _generate_sample(self):\n",
        "    \"\"\"Generates data containing batch_size images\n",
        "    :param list_IDs_temp: list of label ids to load\n",
        "    :return: batch of images\n",
        "    \"\"\"\n",
        "    batch = []\n",
        "    for _ in range(self.batch_size):\n",
        "      try:\n",
        "        batch.append(json.loads(next(self.current_file)))\n",
        "      except (StopIteration, ValueError):\n",
        "        self.current_file.close()\n",
        "        self.current_file = open(self.file_list[0])\n",
        "        self.file_list.rotate()\n",
        "        batch.append(json.loads(next(self.current_file)))\n",
        "    return batch\n",
        "\n",
        "  def _get_total_samples(self):\n",
        "    print(f\"Getting number of samples from all files\")\n",
        "\n",
        "    # Count all samples in specified files\n",
        "    with Pool(4) as pool:\n",
        "      samples_per_file = pool.map(rawgencount, self.file_list)\n",
        "    return sum(samples_per_file)\n",
        "\n",
        "  def prune_sample(self, sample):\n",
        "    \"\"\"Remove all values that do not fit in the vocabulary\n",
        "    \"\"\"\n",
        "    return [x for x in sample if x <= self.vocab_size and x not in stopwords]\n",
        "\n",
        "  def prune_label(self, label):\n",
        "    return [x for x in label if x <= self.answer_vocab]\n",
        "  \n",
        "\n",
        "def _make_gen(reader):\n",
        "  b = reader(1024 * 1024)\n",
        "  while b:\n",
        "    yield b\n",
        "    b = reader(1024*1024)\n",
        "\n",
        "def rawgencount(filename):\n",
        "  print(f\"Opening {filename}\")\n",
        "  with open(filename, 'rb') as f:\n",
        "    f_gen = _make_gen(f.raw.read)\n",
        "    return sum(buf.count(b'\\n') for buf in f_gen )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aElzjmW8uHnP",
        "colab_type": "text"
      },
      "source": [
        "Conversion of bow.py to a Jupyter notebook using Keras + Tensorflow 2.x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MqJFlSO0EAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"FastText Model.\"\"\"\n",
        "\n",
        "\n",
        "tbcallback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=1)\n",
        "\n",
        "def main():\n",
        "  inputs = keras.Input(shape=(MAX_LENGTH+MAX_NGRAMS,), name='input')\n",
        "  embedding = layers.Embedding(VOCAB_SIZE+len(stopwords)+len(ngram_list),\n",
        "                               EMBED_DIM, mask_zero=True)(inputs)\n",
        "  lstm = layers.LSTM(8, activation='linear',\n",
        "                     input_shape=(\n",
        "                         EMBED_DIM, VOCAB_SIZE+len(stopwords)+len(ngram_list)),\n",
        "                     return_sequences=True)(embedding)\n",
        "  average = layers.GlobalAveragePooling1D()(lstm)\n",
        "  outputs = layers.Dense(ANSWER_VOCAB,\n",
        "                         activation=keras.activations.sigmoid)(average)\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "                loss=keras.losses.binary_crossentropy,\n",
        "                metrics=['categorical_accuracy'])\n",
        "  test_generator = DataGenerator(file_prefix='test')\n",
        "  model.fit_generator(test_generator, steps_per_epoch=10,\n",
        "                      epochs=5, verbose=2)\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx0cY2j-G7OL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Generate Ngrams\"\"\"\n",
        "\n",
        "def main():\n",
        "  filenames = [f\"{PATH_PREFIX}/{file}\" for file in glob.glob1(\n",
        "      PATH_PREFIX,f\"{TEST_FILE}*.json\")]\n",
        "  ngram_indices, sorted_ngrams = create_ngram_set(filenames, ngrams=2)\n",
        "  save_ngrams = {\"indices\": ngram_indices, \"sorted\": sorted_ngrams}\n",
        "  with open(f\"{PATH_PREFIX}/test_ngrams.json\", 'w') as savefile:\n",
        "    json.dump(save_ngrams, savefile)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmpMJtAV4-DT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  %tensorboard --logdir logs/fit"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}